{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Classification with the German Traffic Sign Recognition Benchmark\n",
    "\n",
    "<img src=\"img/sv_pp_TrafficSignRecognition.jpg\" align=\"right\" width=200>\n",
    "An automatic road sign recognition system first locates road signs within images captured by an imaging sensor on-board of a vehicle, and then identifies road signs assisting the driver to properly operate the vehicle.\n",
    "\n",
    "Automated road sign recognition is a difficult task. There are a number of important issues that need to be taken into consideration. These include: illumination conditions, direction of sign's face, status of paint on signs, placement of multiple signs near each other, torn and tilted signs, variations in sign's scale, obstacles such as tree, image sensor's properties, car vibrations, etc. \n",
    "\n",
    "Assuming that the road sign has been previously located in the image, neural networks may be employed to implement the classification module because they have proven to be good classifiers and have been able to successfully solve several object recognition problems. \n",
    "\n",
    "In this notebook you will work on a classification task of several road signs with neural networks. The images will be obtained from [a large, lifelike database of traffic sign images: the German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocessing\n",
    "\n",
    "The German Traffic Sign Benchmark is a multi-class, single-image classification challenge, with the following properties:\n",
    "\n",
    "* Single-image, multi-class classification problem\n",
    "* More than 40 classes\n",
    "* More than 50,000 images in total\n",
    "* Large, lifelike database\n",
    "\n",
    "The training set archive is structured as follows:\n",
    "\n",
    "* One directory per class\n",
    "* Each directory contains one CSV file with annotations (\"GT-<ClassID>.csv\") and the training images\n",
    "* Training images are grouped by tracks\n",
    "* Each track contains 30 images of one single physical traffic sign\n",
    "\n",
    "You will work with a small sub-set of this benchmark, by selecting only **one track of each of the following 4 classes**:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/00004_00022.png\"></td>\n",
    "<td><img src=\"img/00012_00026.png\"></td>\n",
    "<td><img src=\"img/00010_00016.png\"></td>\n",
    "<td><img src=\"img/00003_00009.png\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>00003</td>\n",
    "<td>00007</td>\n",
    "<td>00013</td>\n",
    "<td>00014</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "The images can be downloaded from [here](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset#Downloads).\n",
    "\n",
    "The images are PPM images (RGB color). Files are numbered in two parts:\n",
    "\n",
    "    XXXXX_YYYYY.ppm\n",
    "\n",
    "The first part, `XXXXX`, represents the track number. The second part, `YYYYY`, is a running number within the track.\n",
    "\n",
    "The annotations are stored in CSV format (field separator is \";\" (semicolon) ). The annotations contain meta information about the image and the class:\n",
    "\n",
    "* Filename - Image file the following information applies to\n",
    "* Width, Height - Dimensions of the image\n",
    "* Roi.x1,Roi.y1, Roi.x2,Roi.y2 - Location of the sign within the image (Images contain a border around the actual sign of 10 percent of the sign size, at least 5 pixel)\n",
    "* ClassId - The class of the traffic sign\n",
    "\n",
    "The following cell will read tracks 5, 40, 24, and 8 from classes 3, 7, 13, and 14 respectively. The data will be stored in the resulting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from packages.gtrsb import readTrafficSigns\n",
    "\n",
    "classes = [3, 7, 13, 14]\n",
    "tracks = {3: (5, 10, 21, 8, 28), 7: (40, 8, 18, 5, 22), 13: (24,6,33, 11, 29), 14: (8,15,19, 2, 11)}\n",
    "trainImages, trainDims, trainROIs, trainLabels, filenames = readTrafficSigns('./data', classes, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(trainImages))\n",
    "print(trainDims[0])\n",
    "print(trainROIs[0])\n",
    "print(trainLabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "tracks = range(10)\n",
    "random.shuffle(tracks)\n",
    "tracks[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "6 in tracks[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "Please notice the following information about images:\n",
    "\n",
    "* The images contain one traffic sign each\n",
    "* Images contain a border of 10 % around the actual traffic sign (at least 5 pixels) to allow for edge-based approaches\n",
    "* Image sizes vary between 15x15 to 250x250 pixels\n",
    "* Images are not necessarily squared\n",
    "* The actual traffic sign is not necessarily centered within the image.This is true for images that were close to the image border in the full camera image\n",
    "\n",
    "<img src=\"img/GT-Example.png\" align=\"right\" width=100>\n",
    "The ROI is defined as:\n",
    "* `ROI.x1`: X-coordinate of top-left corner of traffic sign bounding box\n",
    "* `ROI.y1`: Y-coordinate of top-left corner of traffic sign bounding box\n",
    "* `ROI.x2`: X-coordinate of bottom-right corner of traffic sign bounding box\n",
    "* `ROI.y2`: Y-coordinate of bottom-right corner of traffic sign bounding box\n",
    "\n",
    "The pixels of the image will be the inputs of the neural network. But the number of inputs is constant, thus you need to make all the images in the dataset to be the same resolution.\n",
    "\n",
    "Moreover, the number of inputs has a significant input on the cost of training. A compromise is needed, by keeping the resolution low while still making possible the distinction of road signs.\n",
    "\n",
    "As a suggestion, you could try with image sizes between 10x10 and 20x20 pixels.\n",
    "\n",
    "In addition, the original images are stored in RGB format, that is, each images has three pixel planes (one for each component). As a recommendation, you might use only the red channel (i.e. a single plane) which seems to contain the most interesting information about the road sign.\n",
    "\n",
    "In summary, the recommended image processing steps are:\n",
    "\n",
    "* Crop the image, i.e. select only the part of the image inside the ROI\n",
    "* Scale the image to a fixed, small resultion\n",
    "* Select the red channel\n",
    "* Adjust the contrast with histogram equalization\n",
    "* Finally, normalize the pixel values to the interval [-1,+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from packages.gtrsb import processImage, plotTrafficSign\n",
    "\n",
    "i = 0\n",
    "roi = trainROIs[i]\n",
    "p1, p2 = roi\n",
    "x1, y1 = p1\n",
    "x2, y2 = p2\n",
    "plt.subplot(121)\n",
    "plt.imshow(trainImages[i])\n",
    "plt.plot([x1,x2,x2,x1,x1],[y1,y1,y2,y2,y1],'c')\n",
    "plt.subplot(122)\n",
    "pr_img = processImage(trainImages[i],trainROIs[i])\n",
    "plt.imshow(pr_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "img = trainImages[i]\n",
    "roi = trainROIs[i]\n",
    "plotTrafficSign(img, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.empty(shape=(0,400))\n",
    "y = []\n",
    "class_labels=map(str,classes)\n",
    "for i in range(len(trainImages)):\n",
    "    eq_img = processImage(trainImages[i],trainROIs[i])\n",
    "    inpt = eq_img.reshape(1,400)\n",
    "    target = class_labels.index(trainLabels[i])\n",
    "    X = np.vstack((X,inpt))\n",
    "    y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = MLPClassifier(solver='sgd',\\\n",
    "                    hidden_layer_sizes=(5, ),\\\n",
    "                    max_iter=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the network\n",
    "\n",
    "### Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected = y_test\n",
    "predicted = net.predict(X_test)\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(net.loss_curve_);\n",
    "plt.xlabel('Iterations');\n",
    "plt.ylabel('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Samples of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
